{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12be9cc3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "text = input(\"Enter a paragraph: \")\n",
    "words = text.split()\n",
    "print(\"\\nSequential Representation:\")\n",
    "for index, word in enumerate(words):\n",
    "    print(f\"Index {index}: {word}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d1ba30",
   "metadata": {},
   "source": [
    "ðŸ”¹ How enumerate() Works in Python\n",
    "Normally, if you loop over a list:\n",
    "\n",
    "python\n",
    "words = [\"Generative\", \"AI\", \"is\", \"powerful\"]\n",
    "for word in words:\n",
    "    print(word)\n",
    "You only get the item (word).\n",
    "But sometimes you also want the position index (0, 1, 2, â€¦).\n",
    "\n",
    "Thatâ€™s where enumerate() comes in:\n",
    "\n",
    "```\n",
    "for index, word in enumerate(words):\n",
    "    print(f\"Index {index}: {word}\")\n",
    "enumerate(words) produces pairs: (0, \"Generative\"), (1, \"AI\"), (2, \"is\"), (3, \"powerful\")\n",
    "```\n",
    "\n",
    "The loop unpacks each *pair into index and word.*\n",
    "So you get both the position and the value at the same time.\n",
    "\n",
    "ðŸ‘‰ Think of enumerate() as a helper that attaches a counter to your list items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b32cdb7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "text = input(\"Enter a paragraph: \")\n",
    "word_tokens = text.split()\n",
    "char_tokens = list(text)\n",
    "print(\"\\nWord-level tokens:\", word_tokens)\n",
    "print(\"Total word tokens:\", len(word_tokens))\n",
    "\n",
    "print(\"\\nCharacter-level tokens:\", char_tokens)\n",
    "print(\"Total character tokens:\", len(char_tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58635810",
   "metadata": {},
   "source": [
    "ðŸ”¹ Why This Matters for LMs\n",
    "Word tokens â†’ capture meaning at the word level.\n",
    "\n",
    "Character tokens â†’ useful for handling unknown words, typos, or morphologies.\n",
    "\n",
    "Modern LMs often use subword tokenization (like Byte Pair Encoding or WordPiece), which is a middle ground between word and character tokens.\n",
    "\n",
    "But for now, word + character tokenization helps you see the granularity of text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241d54ef",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "text = input(\"Enter a paragraph: \")\n",
    "word_tokens = text.split()\n",
    "\n",
    "# Normalize tokens\n",
    "cleaned_tokens = []\n",
    "for w in word_tokens:\n",
    "    w = w.lower()  \n",
    "    w = w.strip(string.punctuation)  \n",
    "    # This removes punctuation like ; , . ! ? from the start/end of the word.\n",
    "    if w:  \n",
    "        cleaned_tokens.append(w)\n",
    "freq = Counter(cleaned_tokens)\n",
    "\n",
    "print(\"\\nTop 10 most frequent tokens:\")\n",
    "for token, count in freq.most_common(10):\n",
    "    # most_common used to find top x ones.\n",
    "    print(f\"{token}: {count}\")\n",
    "\n",
    "print(freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29271bf",
   "metadata": {},
   "source": [
    "ðŸ”¹ Why This Matters for LMs\n",
    "Normalization ensures \"AI\", \"ai\", \"Ai!\" are treated as the same token.\n",
    "\n",
    "Frequency analysis shows statistical patterns in language (some words appear very often, others rarely).\n",
    "\n",
    "This is the foundation of probability-based prediction (next-token models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9c7f23",
   "metadata": {},
   "source": [
    "Build a bigram dictionary:\n",
    "```Key = current word\n",
    "Value = dictionary of next words and their counts```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1744c905",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "# cleaned_tokens = [\"ai\", \"is\", \"powerful\", \"ai\", \"is\", \"exciting\"]\n",
    "\n",
    "bigram_dict = defaultdict(list)\n",
    "print(bigram_dict)\n",
    "\n",
    "for i in range(len(cleaned_tokens) - 1):\n",
    "    current_word = cleaned_tokens[i]\n",
    "    next_word = cleaned_tokens[i + 1]\n",
    "    bigram_dict[current_word].append(next_word)\n",
    "print(bigram_dict)\n",
    "\n",
    "# Step 2: Predict next word\n",
    "user_word = input(\"Enter a word to predict its next word: \").lower()\n",
    "\n",
    "if user_word in bigram_dict:\n",
    "    next_words = bigram_dict[user_word]\n",
    "    freq = Counter(next_words)\n",
    "    predicted_word = freq.most_common(1)[0][0]\n",
    "    print(f\"Most likely next word after '{user_word}' is: {predicted_word}\")\n",
    "else:\n",
    "    print(f\"No prediction possible for '{user_word}' (word not found in text).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97270478",
   "metadata": {},
   "source": [
    "ðŸ”¹ Why This Matters for LMs\n",
    "Modern LMs (like GPT) predict the next token by looking at context.\n",
    "\n",
    "The bigram model is a simplified version: it only looks at the previous word.\n",
    "\n",
    "Real LMs use longer context windows and neural networks, but the principle is the same:\n",
    "\n",
    "ð‘ƒ\n",
    "(\n",
    "nextÂ token\n",
    "âˆ£\n",
    "previousÂ tokens\n",
    ")\n",
    "Trigram model (looking at 2 previous words for prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f4a521",
   "metadata": {},
   "source": [
    "ðŸ”¹ Concept Breakdown\n",
    "So far, weâ€™ve been working with tokens as text. But computers canâ€™t do math with words â€” they need numbers.\n",
    "Thatâ€™s where embeddings come in.\n",
    "\n",
    "Embedding = numeric vector representation of a word.\n",
    "\n",
    "In this question, we use one-hot encoding (the simplest form).\n",
    "\n",
    "One-Hot Encoding\n",
    "Vocabulary size = number of unique words.\n",
    "\n",
    "Each word gets a vector of length = vocabulary size.\n",
    "\n",
    "Only one position is 1, all others are 0.\n",
    "\n",
    "Example:\n",
    "Vocabulary = [\"ai\", \"learns\", \"patterns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0753e6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cleaned_tokens = [\"ai\", \"is\", \"powerful\", \"ai\", \"is\", \"exciting\",\"gc\"]\n",
    "\n",
    "vocab = sorted(set(cleaned_tokens))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Step 2: Create embeddings dictionary\n",
    "embeddings = {}\n",
    "\n",
    "for i, word in enumerate(vocab):\n",
    "    vector = [0] * vocab_size\n",
    "    vector[i] = 1\n",
    "    embeddings[word] = vector\n",
    "\n",
    "print(\"\\nSample embeddings:\")\n",
    "for sample in [\"ai\", \"is\", \"powerful\",\"gc\"]:\n",
    "    if sample in embeddings:\n",
    "        print(f\"{sample}: {embeddings[sample]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc9cf14",
   "metadata": {},
   "source": [
    "ðŸ”¹ Why This Matters for LMs\n",
    "One-hot encoding is the first step in representing words numerically.\n",
    "\n",
    "Real LMs use dense embeddings (vectors with many non-zero values) that capture semantic meaning.\n",
    "\n",
    "But one-hot encoding helps you see the mechanics: words â†’ vectors â†’ math operations possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b157693",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Assume embeddings dictionary is already built from Q5\n",
    "# Example: embeddings = {\"ai\":[1,0,0,0], \"is\":[0,1,0,0], \"powerful\":[0,0,1,0], \"exciting\":[0,0,0,1]}\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot = sum(a*b for a, b in zip(vec1, vec2))\n",
    "    mag1 = math.sqrt(sum(a*a for a in vec1))\n",
    "    mag2 = math.sqrt(sum(b*b for b in vec2))\n",
    "    if mag1 == 0 or mag2 == 0:\n",
    "        return 0\n",
    "    return dot / (mag1 * mag2)\n",
    "\n",
    "# Accept two words\n",
    "word1 = input(\"Enter first word: \").lower()\n",
    "word2 = input(\"Enter second word: \").lower()\n",
    "\n",
    "if word1 in embeddings and word2 in embeddings:\n",
    "    sim = cosine_similarity(embeddings[word1], embeddings[word2])\n",
    "    print(f\"Cosine similarity between '{word1}' and '{word2}': {sim:.2f}\")\n",
    "    if sim > 0.5:\n",
    "        print(\"The words are considered similar.\")\n",
    "    else:\n",
    "        print(\"The words are not similar.\")\n",
    "else:\n",
    "    print(\"One or both words not found in vocabulary.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
